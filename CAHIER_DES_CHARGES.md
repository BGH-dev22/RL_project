# CAHIER DES CHARGES
## Projet ProRL : Deep Reinforcement Learning Hi√©rarchique avec M√©moire √âpisodique

---

## üìã INFORMATIONS G√âN√âRALES

| √âl√©ment | Description |
|---------|-------------|
| **Nom du Projet** | ProRL (Progressive Reinforcement Learning) |
| **Type** | Projet de Recherche Appliqu√©e en Intelligence Artificielle |
| **Domaine** | Deep Reinforcement Learning |
| **Date** | D√©cembre 2025 |

---

## üéØ OBJECTIFS DU PROJET

### Objectif Principal
D√©velopper et √©valuer une architecture de Deep Q-Network (DQN) am√©lior√©e combinant plusieurs techniques avanc√©es pour r√©soudre des probl√®mes √† r√©compenses rares et t√¢ches s√©quentielles complexes.

### Objectifs Sp√©cifiques

1. **Impl√©menter et comparer 6 variantes DQN** :
   - DQN Vanilla (baseline)
   - DQN avec Prioritized Experience Replay (PER)
   - DQN avec M√©moire √âpisodique
   - DQN Hi√©rarchique
   - DQN Full (combinaison compl√®te)
   - DQN Full + Explainability

2. **Proposer des innovations techniques** :
   - M√©moire √©pisodique adaptative avec similarit√© contextuelle
   - Analyse th√©orique des synergies entre composants
   - Module de Transfer Learning

3. **Appliquer √† un probl√®me r√©el** :
   - Environnement de robotique d'entrep√¥t (style Amazon Robotics)

---

## üî¨ PROBL√âMATIQUE

### Contexte
Le Deep Reinforcement Learning souffre de plusieurs limitations dans les environnements √† r√©compenses rares :
- **Exploration inefficace** : L'agent peine √† d√©couvrir les √©tats r√©compensants
- **Oubli catastrophique** : Les bonnes exp√©riences sont perdues
- **Complexit√© des t√¢ches s√©quentielles** : Difficult√© √† d√©composer les objectifs

### Questions de Recherche
1. Comment combiner efficacement PER, m√©moire √©pisodique et architecture hi√©rarchique ?
2. Quelles synergies existent entre ces techniques ?
3. Les comp√©tences apprises se transf√®rent-elles √† de nouveaux environnements ?
4. Comment appliquer ces techniques √† un probl√®me de robotique r√©el ?

---

## üèóÔ∏è ARCHITECTURE TECHNIQUE

### 1. Environnements

#### GridWorld (Environnement de Base)
```
Taille : 10x10
√âl√©ments : Murs, Pi√®ges, Cl√©, Porte, Goal
T√¢che : Collecter cl√© ‚Üí Ouvrir porte ‚Üí Atteindre goal
Actions : UP, DOWN, LEFT, RIGHT, USE_KEY
```

#### Warehouse Robot (Environnement R√©el)
```
Taille : 20x15
√âl√©ments : √âtag√®res, Zones pickup/dropoff, Chargeur, Autres robots
T√¢che : Ramasser colis ‚Üí Livrer ‚Üí G√©rer batterie
Actions : UP, DOWN, LEFT, RIGHT, PICKUP, DROP, CHARGE, WAIT
```

### 2. Agents Impl√©ment√©s

| Agent | Description | Complexit√© |
|-------|-------------|------------|
| **DQN Vanilla** | R√©seau Q standard avec replay buffer | O(1) |
| **DQN + PER** | Replay prioris√© par erreur TD | O(log n) |
| **DQN + Memory** | M√©moire √©pisodique pour stocker trajectoires r√©ussies | O(n) |
| **DQN Hi√©rarchique** | Deux niveaux : sous-objectifs + actions | O(2n) |
| **DQN Full** | Combinaison de toutes les techniques | O(2n log n) |

### 3. Composants Innovants

#### M√©moire √âpisodique Adaptative (AEM-CS)
- Similarit√© contextuelle multi-crit√®res
- Clustering automatique par pattern de succ√®s
- Reconstruction de trajectoires optimales
- Meta-learning pour param√®tres adaptatifs

#### Module d'Analyse Th√©orique
- Calcul des synergies entre composants
- Estimation des bornes de convergence
- M√©triques de complexit√© d'√©chantillonnage

#### Transfer Learning
- Extraction de skills depuis agents entra√Æn√©s
- Zero-shot et few-shot transfer
- √âvaluation sur environnements modifi√©s

---

## üìä R√âSULTATS OBTENUS

### Comparaison des Variantes (GridWorld - 3000 √©pisodes)

| Variante | Cl√©s (%) | Portes (%) | Goals (%) | Retour Moyen | Convergence |
|----------|----------|------------|-----------|--------------|-------------|
| vanilla | 96.5% | 75.8% | 66.6% | -102.1 | 1.00x |
| per | 97.5% | 73.1% | 58.4% | -128.0 | 1.25x |
| memory | 96.6% | 75.5% | 67.9% | -86.9 | 1.33x |
| hier | 98.0% | 89.6% | 71.3% | -34.3 | 1.43x |
| full | 97.3% | 89.1% | **72.8%** | -42.5 | **4.55x** |
| full_explain | 98.0% | 90.6% | 68.2% | -43.2 | 4.55x |

### Robot d'Entrep√¥t (1000 √©pisodes)

| M√©trique | D√©but (Ep. 50) | Fin (Ep. 1000) | Am√©lioration |
|----------|----------------|----------------|--------------|
| Retour moyen | -161.7 | +58.1 | **+220** |
| Colis livr√©s | 0.00/3 | 1.60/3 | **+53%** |
| Missions compl√®tes | 0% | 18% | **+18%** |
| Mort batterie | 100% | 12% | **-88%** |

### Analyse Th√©orique des Synergies

| Combinaison | Synergie Th√©orique | Recommandation |
|-------------|-------------------|----------------|
| PER + M√©moire √âpisodique | 0.215 | ‚úÖ COMBINER |
| M√©moire + Hi√©rarchique | 0.375 | ‚úÖ COMBINER (optimal) |
| PER + Hi√©rarchique | 0.069 | ‚ö™ OPTIONNEL |

---

## üõ†Ô∏è TECHNOLOGIES UTILIS√âES

### Langages et Frameworks
- **Python 3.11** : Langage principal
- **PyTorch** : Framework deep learning
- **NumPy** : Calculs num√©riques
- **Matplotlib** : Visualisations

### Architecture Logicielle
```
ProRL/
‚îú‚îÄ‚îÄ agents/                    # Impl√©mentations des agents
‚îÇ   ‚îú‚îÄ‚îÄ dqn_base.py           # DQN vanilla + PER
‚îÇ   ‚îú‚îÄ‚îÄ hierarchical_dqn.py   # DQN hi√©rarchique
‚îÇ   ‚îú‚îÄ‚îÄ episodic_memory.py    # M√©moire √©pisodique standard
‚îÇ   ‚îî‚îÄ‚îÄ adaptive_episodic_memory.py  # [INNOVATION] M√©moire adaptative
‚îú‚îÄ‚îÄ env/                       # Environnements
‚îÇ   ‚îú‚îÄ‚îÄ gridworld.py          # GridWorld cl√©-porte-goal
‚îÇ   ‚îî‚îÄ‚îÄ warehouse_robot.py    # [INNOVATION] Robot d'entrep√¥t
‚îú‚îÄ‚îÄ experiments/               # Scripts d'exp√©rimentation
‚îÇ   ‚îú‚îÄ‚îÄ compare_variants.py   # Comparaison des 6 variantes
‚îÇ   ‚îú‚îÄ‚îÄ train_warehouse.py    # Entra√Ænement robot
‚îÇ   ‚îú‚îÄ‚îÄ transfer_learning.py  # [INNOVATION] Transfer learning
‚îÇ   ‚îî‚îÄ‚îÄ run_innovations.py    # Script principal innovations
‚îú‚îÄ‚îÄ analysis/                  # Modules d'analyse
‚îÇ   ‚îú‚îÄ‚îÄ theoretical_analysis.py  # [INNOVATION] Analyse th√©orique
‚îÇ   ‚îî‚îÄ‚îÄ plots.py              # G√©n√©ration graphiques
‚îú‚îÄ‚îÄ explainability/            # Explicabilit√©
‚îÇ   ‚îî‚îÄ‚îÄ trajectory_attribution.py
‚îî‚îÄ‚îÄ results/                   # R√©sultats et m√©triques
```

---

## üöÄ CONTRIBUTIONS ORIGINALES

### 1. M√©moire √âpisodique Adaptative (AEM-CS)
**Innovation technique** : Am√©lioration de la m√©moire √©pisodique classique avec :
- Similarit√© contextuelle multi-dimensionnelle
- Clustering automatique des √©pisodes
- Reconstruction de trajectoires √† partir de segments r√©ussis
- Auto-ajustement des param√®tres via meta-learning

### 2. Analyse Th√©orique des Synergies
**Contribution scientifique** : Framework formel pour :
- Quantifier les synergies entre composants DQN
- Pr√©dire les gains de performance des combinaisons
- Estimer les bornes de convergence

### 3. Application Robotique R√©elle
**Contribution pratique** : Environnement r√©aliste inspir√© d'Amazon Robotics :
- Multi-objectifs (pickup ‚Üí delivery)
- Contraintes de ressources (batterie)
- Obstacles dynamiques (autres robots)
- 8 actions avec s√©mantique riche

### 4. Transfer Learning pour RL Hi√©rarchique
**Contribution m√©thodologique** :
- Extraction et r√©utilisation de skills
- √âvaluation zero-shot et few-shot
- Protocole de benchmark standardis√©

---

## üìà LIVRABLES

### Code Source
- [x] 6 variantes d'agents DQN fonctionnelles
- [x] 2 environnements (GridWorld + Warehouse)
- [x] Scripts d'entra√Ænement et d'√©valuation
- [x] Module d'analyse th√©orique
- [x] Module de transfer learning

### Documentation
- [x] README.md complet
- [x] Cahier des charges (ce document)
- [x] Commentaires dans le code

### R√©sultats
- [x] M√©triques JSON pour toutes les variantes
- [x] Graphiques de performance
- [x] Rapport d'analyse th√©orique

---

## üîÆ PERSPECTIVES ET EXTENSIONS

### Court Terme
- Augmenter le nombre d'√©pisodes d'entra√Ænement pour le robot
- Ajouter des environnements de warehouse plus complexes
- Impl√©menter un mode multi-robot coop√©ratif

### Moyen Terme
- Int√©grer des techniques de curiosit√© (ICM, RND)
- Ajouter un vrai module Go-Explore
- D√©ployer sur un simulateur robotique (Gazebo, PyBullet)

### Long Terme
- Transfer vers robot r√©el (ROS)
- Apprentissage continu et lifelong learning
- Extension √† d'autres domaines (jeux vid√©o, trading)

---

## üìö R√âF√âRENCES BIBLIOGRAPHIQUES

1. **Mnih et al. (2015)** - "Human-level control through deep reinforcement learning" - Nature
2. **Schaul et al. (2015)** - "Prioritized Experience Replay" - ICLR
3. **Blundell et al. (2016)** - "Model-Free Episodic Control" - ICML
4. **Kulkarni et al. (2016)** - "Hierarchical Deep Reinforcement Learning" - NeurIPS
5. **Vezhnevets et al. (2017)** - "FeUdal Networks for Hierarchical RL" - ICML
6. **Ecoffet et al. (2021)** - "Go-Explore" - Nature

---

## ‚úÖ CONCLUSION

Le projet ProRL d√©montre avec succ√®s que la combinaison de techniques avanc√©es de Deep RL (PER, m√©moire √©pisodique, architecture hi√©rarchique) produit des synergies significatives. Les contributions originales en termes de m√©moire adaptative, d'analyse th√©orique et d'application robotique conf√®rent au projet une valeur ajout√©e au-del√† de la simple reproduction de l'√©tat de l'art.

**Points forts :**
- Am√©lioration de +6.2% du taux de succ√®s (full vs vanilla)
- Convergence 4.55x plus rapide avec la combinaison compl√®te
- Application r√©ussie √† un probl√®me de robotique r√©aliste
- Framework th√©orique pour guider les combinaisons de techniques

---

*Document g√©n√©r√© le 24 d√©cembre 2025*
*Projet ProRL - Deep Reinforcement Learning*
